\documentclass[thesis.tex]{subfiles}

\begin{document}

\chapter{Including all CIS individuals in survival analysis} \label{appendix:total-model}

%\todo[inline]{Add an intro to this and maybe a plot of results in simulation}

We now consider all $N$ individuals in the CIS cohort who have at
least one test during the period of interest, this includes those
without an observed infection, and assume that each is equally likely to
be infected. As before, $i = 1, \dots, n_a$ indexes episodes but now
$j = 1, \dots, N$ indexes individuals. All episodes have an associated
individual, each individual may have any number (including 0) episodes.
Admissible regions are only defined for episodes, and have not changed
from the previous section. The truncated region is defined for each
individual, in addition we consider the combined truncated region which
is episodes that would be truncated without conditioning on which
individual the episode occurs in:
$\Omega^C = \{ (b, e, j) : (b, e) \in \Omega_j^C \}$.

Denote by $n_\text{tot}$ the total number of episodes across all $N$
individuals, regardless of whether they are admissible or truncated (we
know there are no inadmissible episodes by definition).

For any infection, the probability of it being admissible for episode
$i$ is $\frac{1}{N} p_{ia}$. That is, the probability that the
infection occurs in the individual corresponding to the individual in
which episode $i$ ($1/N$ by the assumption that episodes are equally
likely to occur in any of the $N$ individuals) multiplied by the
probability that the episode is admissible for episode $i$ conditional
on it occurring in the relevant individual ($p_{ia}$).

The total number of truncated infections that occur is
$n_t = n_\text{tot} - n_a$. Conditional on an infection occurring in
individual $j$, its probability of being truncated is $p_{jt}$ (as
defined previously). Therefore, the overall probability of an episode
being truncated is $p_t = \frac{1}{N} \sum_{j=1}^N p_{jt}$.

Consider our data as there being one admissible episode for each
observed episode, and all other episodes being truncated. As in the
previous section, conditional on $n_\text{tot}$, we have a multinomial
likelihood.
\begin{align}
p(\text{data} \mid \theta)
&= \sum_{n_\text{tot}=n_a}^\infty p(\text{data} \mid n_\text{tot}, \theta) p(n_\text{tot}) \\
&= \sum_{n_\text{tot}=n_a}^\infty \frac{n_\text{tot}!}{(n_\text{tot}-n_a)!} \left( \prod_{i=1}^{n_a} \frac{1}{N} p_{ia} \right) p_t^{n_\text{tot}-n_a} p(n_\text{tot}) \\
&= \left( \prod_{i=1}^{n_a} \frac{1}{N} p_{ia} \right) \left( \sum_{n_\text{tot}=n_a}^\infty \frac{n_\text{tot}!}{(n_\text{tot}-n_a)!} p_t^{n_\text{tot}-n_a} p(n_\text{tot}) \right) \\
\end{align}

In this case, it would be computationally feasible to augment the data
with $n_\text{tot}$ directly (as it is only a single parameter).
However, implementation is easier if we can use a closed-form for the
sum (as in the previous section) as it allows the use of standard
implementations of NUTS such as Stan, which cannot handle discrete
parameters.

Assuming $N_\text{tot} \dist \NegBin(\mu, r)$:
\begin{align}
&\sum_{n_\text{tot}=n_a}^\infty \frac{n_\text{tot}!}{(n_\text{tot}-n_a)!} p_t^{n_\text{tot}-n_a} p(n_\text{tot}) \\
&= \int \sum_{n_\text{tot}=n_a}^\infty \frac{n_\text{tot}!}{(n_\text{tot}-n_a)!} p_t^{n_\text{tot}-n_a} p(n_\text{tot} \mid \lambda) p(\lambda) d\lambda \\
&= \int \sum_{n_\text{tot}=n_a}^\infty \frac{n_\text{tot}!}{(n_\text{tot}-n_a)!} p_t^{n_\text{tot}-n_a} \frac{\lambda^n_\text{tot} e^{-\lambda}}{n_\text{tot}!} p(\lambda) d\lambda \\
&= \int \sum_{n_\text{tot}=n_a}^\infty \frac{1}{(n_\text{tot}-n_a)!} p_t^{n_\text{tot}-n_a} \lambda^{n_\text{tot}-n_a} \lambda^{n_a} e^{-\lambda} p(\lambda) d\lambda \\
&= \int \lambda^{n_a} e^{-\lambda} p(\lambda) \sum_{n_t=0}^\infty \frac{1}{n_t!} p_t^{n_t} \lambda^{n_t} d\lambda &n_t = n-n_a\\
&= \int \lambda^{n_a} e^{-\lambda} p(\lambda) e^{\lambda p_t} d\lambda \\
&= \int \lambda^{n_a} e^{-\lambda(1 - p_t)} p(\lambda) d\lambda \\
&= \int \lambda^{n_a} e^{-\lambda(1 - p_t)} \frac{b^a}{\Gamma(a)} \lambda^{a-1} e^{-b\lambda} \lambda d\lambda \\
&= \int \frac{b^a}{\Gamma(a)} \lambda^{a+n_a-1} e^{-(b+1-p_t)\lambda} \lambda d\lambda \\
&= \frac{b^a}{\Gamma(a)} \frac{\Gamma(a+n_a)}{(b+1-p_t)^{a+n_a}} \\
&\propto (b+1-p_t)^{-(a+n_a)} \\
&= (r/\mu + 1 - p_t)^{-(r+n_a)} \\
&\propto(r + \mu (1- p_t))^{-(r+n_a)}
\end{align}

Which gives the full posterior density as:
\begin{align}
p(\theta \mid \text{data})
&\propto \frac{p(\theta) \prod_{i=1}^{n_a} p_{ia}}{(r + \mu(1 - p_t))^{r+n_a}}
\end{align}

As before, being able to reconstruct the posterior of $n_\text{tot}$
using its full conditional is useful.
\begin{align}
p(n_\text{tot} \mid \text{data}, \theta)
&\propto p(\text{data} \mid \theta, n_\text{tot}) p(n_\text{tot}) \\
&= \frac{n_\text{tot}!}{(n_\text{tot}-n_a)!} \left( \prod_{i=1}^{n_a} \frac{1}{N} p_{ia} \right) p_t^{n_\text{tot}-n_a} p(n_\text{tot}) \\
&\propto \frac{n_\text{tot}!}{(n_\text{tot}-n_a)!} p_t^{n_\text{tot}} \frac{\Gamma(r + n_\text{tot})}{n_\text{tot}!} \left( \frac{\mu}{r + \mu} \right)^{n_\text{tot}}  \\
&\propto \frac{\Gamma(r + n_\text{tot})}{(n_\text{tot}-n_a)!} \left( \frac{\mu p_t}{r + \mu} \right)^{n_\text{tot}}  \\
&\propto \frac{\Gamma((r + n_a) + (n_\text{tot}- n_a))}{(n_\text{tot}-n_a)!} \left( \frac{\mu p_t}{r + \mu} \right)^{n_\text{tot}-n_a}  \\
\end{align}
which is a negative binomial pmf with size parameter $r + n_a$ and
probability parameter $\frac{r+\mu(1-p_t)}{r+\mu}$.

\end{document}