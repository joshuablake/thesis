\documentclass[thesis.tex]{subfiles}

\begin{document}
\ifSubfilesClassLoaded{
  \setcounter{chapter}{2}
}

\chapter{Statistical and epidemiological background} \label{inc-prev}

%\todo[inline]{This chapter is pretty short. Not sure if this matters or it needs to merged with another chapter? For example, one background chapter split into epi/biology and stats.}

This chapter defines incidence and prevalence, and the relationship between these quantities.
In the course of explaining this relationship, it will become clear that the duration of PCR positivity, and its distribution in the population, is the vital parameter which will allow estimation of incidence from prevalence, a major aim of this thesis.

I start by defining these terms in \cref{inc-prev:sec:definitions}.
I then move on to explaining the three processes that are involved: the \emph{infection process}, the \emph{prevalence process} and the \emph{observation process}.
The infection process (\cref{inc-prev:sec:infection-process}) models how infections arise.
The concepts here are well-established within infectious disease epidemiology (see\todo{ref intro chapter}).
The prevalence process (\cref{inc-prev:sec:prevalence-process}) describes the relationship between incidence and prevalence.
Here, I adapt previous work to the SARS-CoV-2 context.
In particular, the recoveries need explicit modelling which was not required for HIV/AIDS.
The observation process describes the relationship between prevalence and the observed data.
Unlike in HIV/AIDS backcalculation, the CIS has a sampling mechanism that needs to be modelled.
This model is guided by the specific study design of the CIS.
Therefore, the model is specific to this context, although much would be applicable to other, similarly designed prevalence surveys.
Finally, \cref{inc-prev:sec:MCMC} will introduce Markov chain Monte Carlo (MCMC), an inference technique I will make extensive use of in this thesis, before \cref{inc-prev:sec:conclusion} concludes the chapter.

\section{Definitions} \label{inc-prev:sec:definitions}

Time will be considered discrete in this chapter.
I use days, however, the results generalise in the obvious way to other time units.
I make this choice for pragmatic reasons.
Most epidemiological data is only available at daily granularity or, when more granular data exists, is often unreliable.
Furthermore, within-day patterns produce variations in transmission (\eg due to sleep) and data (\eg due to logistical considerations around collection); these variations are not of interest in most contexts.

\emph{Incidence} on day $t$, $Z_t$, is the number of people in a given population who become infected on day $t$; when expressed as a proportion of the population it is the \emph{incidence proportion}~\autocite[89]{lashModern}, $Z_t/\Npop$, where $\Npop$ is the number of people in the population.
As discussed in\todo{ref intro}, the timing of infection events is very rarely directly observable and must be inferred from other data.
Despite this issue, it remains the single most important quantity for informing the response to an epidemic or pandemic.

\emph{Prevalence} of a disease on day $t$, is the number of people with the disease on that day~\autocite[90]{lashModern}.
What it means to have a disease can vary by context (see\todo{ref section discussing what it means to have covid}).
I define an individual to have SARS-CoV-2 on day $t$ if, had they been swabbed on day $t$, that swab would have returned a positive PCR test.
I refer to these individuals as \emph{prevalent} individuals.
I assume that an individual's prevalent status cannot change over the course of a day.
Therefore, prevalence is constant over any single day $t$.
I denote the number of prevalent individuals on day $t$ as $P_t$.
Therefore, the prevalence is $P_t/\Npop$.

Duration, or time-to-event, distributions can be expressed in several equivalent ways.
The duration distribution I will be concerned with is the distribution of the number of days for which an individual is prevalent.
Consider an individual $i$ infected on day $B_i$ that is prevalent during the interval $[B_i, E_i]$.
I define this period as an \emph{infection episode}, beginning on day $B_i$ and ending on day $E_i$.
$D_i = E_i - B_i + 1$ is their \emph{duration of positivity}, the number of days for which they are prevalent.
I model each $D_i$ as a discrete independent and identically distributed (iid) random variable; extending the methodology to account for covariates is left to further work.
The duration is a positive integer, \ie $D_i \in \{1, 2, \dots\}$.
The first way of expressing the distribution of $D_i$ is as the standard probability mass function (pmf), $f_{D}(d) = \prob(D = d)$; or cumulative density function (CDF), $F_{D}(d) = \prob(D \leq d) = \sum_{i=0}^d f_{D}(i)$.
The \emph{survival function} gives the probability that the duration is at least $d$ days: $S(d) = \prob(D \geq d) = 1 - F_{D}(D - 1)$ with $S(1) = 1$~\autocite{yanDistribution}.
The \emph{hazard} at time $t$ is the probability of recovering at time $t$ conditional on having survived to time $t$: $\lambda(t) = \prob(D = t \mid D \geq t) = f_{D}(d) / S(d)$.
For discrete variables, the convention that the survival function is $\prob(D >d)$, rather than $\prob(D \geq d)$, is also commonly used, I adapt that latter convention.
Throughout this thesis, the survival function will be the primary quantity of interest, and it will normally be parameterised in terms of the hazard; these two quantities are related by $S(t) = \prod_{i=1}^{t-1} (1 - \lambda(i))$.


\section{Infection process} \label{inc-prev:sec:infection-process}
The infection process is a \emph{point} or \emph{counting} process, where each infection is an event.

A point process is a random function of continuous time $K(t)$.
$K(t)$ must take non-negative integer values and be non-decreasing in $t$.
The interpretation of $K(t)$ is the number of events, here infections, that have occurred in the interval $(-\infty, t]$~\autocites[244]{yanDistribution}.
In the context of a disease, the point process is discretised into incidence for integer $t$ as $Z_t = K(t) - K(t-1)$.

A counting process has \emph{independent increments} if the number of events in disjoint intervals are independent~\autocite[244]{yanDistribution}.
That is, for any $t_1 < t_2 < t_3 < t_4$, $K(t_2) - K(t_1)$ and $K(t_4) - K(t_3)$ are independent.

A commonly-used point process for infectious disease epidemiology is the time-inhomogenous Poisson process~\autocites{brookmeyerMethod}{paganoHIV}{rosenbergBackcalculation}{brookmeyerBackcalculation}.
A Poisson process is a point process with independent increments, and the property that $K(s) - K(t) \dist \Poi\left(\int^s_t \lambda(r) dr \right)$ where $\lambda(r) \geq 0$ is the \emph{intensity} of the epidemic at time $r$~\autocites[244]{yanDistribution}.
A time-homogenous Poisson process is the special case where $\lambda(r)$ is constant.
The independent increment property says that, conditional on $\lambda(r)$, the number of infections in disjoint intervals are independent.
This implies $Z_t$ and $Z_{t'}$ are independent for $t \neq t'$.
Extensions to this model can incorporate covariates, such as geography or age~\autocite[e.g.][]{diggleModeling}.

The target of inference can be either the process's intensity or the counts of the specific realisation that occurred.
The realised counts are normally of greater interest because we care about the realised epidemic, not what may happen if there were future realisations of the same epidemic~\autocites{beckerDependent}{brookmeyerMethod}.

Two features of a Poisson process are violated for epidemics.
Epidemics commonly exhibit overdispersion and have feedback loops~\autocite{beckerDependent}.
Overdispersion means that $\V(Z_t) > \E(Z_t)$ for any day $t$.
In a Poisson process these counts have a Poisson distribution, hence, $\V(Z_t) = \E(Z_t)$.
Feedback loops mean that the stochastically low or high numbers of infections, possibly due to overdispersion, affect the future number of infections.
This violates the assumption of independent increments.
In particular, $\lambda(t+\delta t)$ is dependent on $K(t)$ for $\delta t > 0$.
Both of these features violate the properties of a Poisson process.
Despite these issues, consistent inference on the incidence can still be produced using the Poisson process model, although the uncertainty in the estimates may be understated~\autocite{beckerDependent}.

A common cause of overdispersion is super-spreading events~\autocite{lloyd-smithSuperspreading}.
A super-spreading event is an event where many more people than average are infected.
That is, $Z_t >> \E(Z_t)$ for some $t$ during which a super-spreading event occurs.
Coronaviruses, such as SARS-CoV-2, have particularly high overdispersion~\autocites{endoEstimating}{adamClustering}{mccloskeySARS}.
An infamous example of a SARS-CoV-2 super-spreading event was the Skagit County choir practice in March 2020~\autocite{hamnerHigh}.
One infected individual is thought to have infected at least 32 further individuals during a single choir practice.
For comparison, the average individual infected around three others in the early stages of the pandemic~\autocite{pellisChallenges}. 
Overdispersion can be incorporated by replacing the assumption that $K(s) - K(t)$ is Poisson distributed with assuming it is negative binomially distributed.
This would not affect $\E(K(s) - K(t))$, but the negative binomial introduces an additional overdispersion parameter, commonly notated $k$.
%The negative binomial tends to Poisson as $k\to\infty$.
Generally, $k<1$ is considered as significant levels of overdispersion.
Estimates for SARS-CoV-2 indicate $k \approx 0.1$~\autocite{endoEstimating}, and may be decreased by some interventions including lockdowns~\autocites{quiltyReconstructing}{quiltyUnderstanding}.
If $K(s) - K(t)$ has a negative binomial distribution, then its standard deviation scales on the order of the mean.
If it is Poisson distributed, then the standard deviation is on the order of the square root of the mean.
A particular implication of this is that the coefficient of variation (the standard deviation divided by the mean) tends to zero for a Poisson process, but tends to a constant for a negative binomial process.
The disadvantage of the negative binomial distribution is that it loses much of the mathematical convenience of a Poisson distribution.

The independence assumption of a Poisson or negative binomial process is violated because if the number of infections is stochastically high (or low), this feeds back to the process intensity at future times.
This can be seen simply in the event of a super-spreading event.
If a super-spreading event occurs, then many more individuals than suggested by the process intensity will be infected.
These additional infections will propagate the epidemic further, increasing the intensity at future times.
Point processes can be extended to \emph{branching processes} to account for this feedback~\autocite[246]{yanDistribution}, although these are beyond the scope of this thesis.

An alternative to explicit use of counting processes is using \emph{mechanistic models}.
Mechanistic models explicitly describe the population and transmission between individuals.
They vary greatly in their details and realism~\autocite{murilloMultiscale}.
Mechanistic models are discussed further in \cref{E-SEIR}.

% In \emph{deterministic backcalculation}, the observed prevalence, $x_t/n_t$, is assumed to be equal to $P_t/\Npop$.
% However, this is often a poor approximation.
% \emph{Statistical backcalculation} retains the sampling distribution of $x_t$ (or an approximation of it, such as a Poisson).

% Define the first day of an infection episode in individual $i$ as $B_i$, and assume that the probability of $i$ being infected multiple times within the period of interest is negligible.
% The time between being infected and first being detectable is short (see\todo{ref relevant part}), and therefore I often assume that $B_i$ is the same as the time of infection.
% I keep this assumption for the remainder of this chapter.


% Backcalculation makes use of this relationship to estimate incidence from prevalence, assuming that the duration is known.

% Deterministic backcalculation assumes that the variance in the population is negligible, and therefore that the prevalence is equal to the mean prevalence.
% This is justified because the variance due to sampling is much larger than the variance due to the true prevalence.

\section{Prevalence process} \label{inc-prev:sec:prevalence-process}

The relationship between incidence and prevalence can be derived by considering the probability that an individual is prevalent at any time following an infection.
%I define the period the individual is prevalent for as their \emph{infection episode}.
%I index infection episodes with $i$.
%Denote the time that the infection episode begins as $B_i$ and the time it ends as $E_i \geq B_i$.
%The individual is prevalent during the interval $[B_i, E_i]$ (\ie including $B_i$ and $E_i$).
%The duration of the infection episode, the number of days for which they are prevalent, is $D_i = E_i - B_i + 1$.
%I assume that the $D_i$s are iid, with a discrete distribution defined by $\prob(D_i = d) = f_D(d)$ for $d = 1, \dots, d_\text{max}$, for some maximum episode length $d_\text{max}$, and $\prob(D_i = d) = 0$ otherwise.
%Denote by $F_D$ the cdf of $D_i$, that is $F_D(d) = \prob(D_i \leq d)$.
From this, we can derive the relationship between incidence and prevalence.

Denote by $R_{t,t'}$ be the number of infection episodes with $B_i=t$ and $E_i=t'$.
For each infection beginning at $t$, the probability that it ends at $t'$ is $\prob(E_i = t' \mid B_i = t) = \prob(D_i = t' - t + 1 \mid B_i = t) = f_D(t' - t + 1)$.
Since these durations are assumed independent, the number of episodes of each duration is multinomially distributed~\autocite{paganoHIV}.
Therefore:
\begin{align}
\begin{bmatrix}
  R_{t,t} \\ R_{t,t+1} \\ \vdots \\ R_{t,t+d_\text{max}-1}
\end{bmatrix} \mid Z_t
\sim \MN \left(
  Z_t, 
  \begin{bmatrix}
    f_D(1) \\ f_D(2) \\ \vdots \\ f_D(d_\text{max})
  \end{bmatrix}
\right).
\end{align}
Now, denote by $P_{t_z,t_p}$ the number of infection episodes with $B_i = t_z$ and $E_i \geq t_p$.
Therefore, the individuals in which these infection episodes occur are prevalent at time $t_p$.
$P_{t_z,t_p}$ is $Z_{t_z}$ minus those that recover before $t_p$ (\ie at or up to $t_p - 1$).
Hence:
\begin{align}
    P_{t_z,t_p} = \begin{cases}
      0 &t_p < t_z\\
      Z_{t_z} - \sum_{i=0}^{t_p-t_z-1} R_{t_z,t_z+i} &t_z \leq t_p < t_z + d_\text{max}\\
      0 &t_p \geq t_z + d_\text{max}.
  \end{cases} \label{inc-prev:eq:Ptt-to-R}
\end{align}
Finally, the $P_t$ can be expressed as the total number of people prevalent at time $t$, regardless of when they were infected.
The limits of the sum are limited by the zeroes in \cref{inc-prev:eq:Ptt-to-R}.
\begin{align}
  P_t
  &= \sum_{t_z=-\infty}^\infty P_{t_z,t} \\
  &= \sum_{i=0}^{\dmax-1} P_{t-i,t} \label{inc-prev:eq:Pt-to-Ptt} \\
  &= \sum_{i=0}^{\dmax-1} \left(Z_{t_z} - \sum_{j=0}^{i-1} R_{t-i,t-i+j} \right) &\text{by \cref{inc-prev:eq:Ptt-to-R}}\label{inc-prev:eq:Pt-to-Rtt}.
\end{align}

In general, all the random variables in this section are dependent.
This is because they are linked by the $Z_t$s, which are themselves dependent (as explained in \cref{inc-prev:sec:infection-process}).
However, if we condition on the vector of incidence, $\vec{Z} = (Z_1, Z_2, \dots)^T$, then we will induce some independence.
This reduces the stochasticity to only that attributable to the prevalence process.
In particular, because the duration of individuals is independent, $R_{t_z,t_p}$ and $R_{t_z',t_p'}$ will be independent if $t_z \neq t_z'$; importantly, this holds even if $t_p = t_p'$.
Therefore, $P_{t_z,t_p}$ and $P_{t_z',t_p'}$ will be independent if $t_z \neq t_z'$.

In \cref{inc-prev:sec:observation-process} I will show that only $\E(P_t \mid \vec{Z})$ and bounds on $\V(P_t \mid \vec{Z})$ will be relevant.
These can be derived from their constituent parts.

Start by considering the recoveries.
The sum of recoveries of infections that occurred on the same day $t$ is the sum of multinomial cell probabilities, and hence binomially distributed~\autocite{alamAnalysis}.
Specifically:
\begin{align}
  \sum_{i=0}^{t'} R_{t,t+i} \mid \vec{Z} &\sim \text{Binomial}(Z_t, F_D(t'+1)). \label{inc-prev:eq:binomialRt}
\end{align}
where $F_D(t) = \sum_{i=1}^t f_D(t) = \prob(D \leq t)$ is the CDF of $D$.
Therefore:
\begin{align}
  \E \left( \sum_{i=0}^{t'} R_{t,t+i} \mid \vec{Z} \right) &= Z_t F_D(t'+1) \label{inc-prev:eq:EsumRt} \\
  \V \left( \sum_{i=0}^{t'} R_{t,t+i} \mid \vec{Z} \right) &= Z_t F_D(t'+1) (1 - F_D(t'+1)) \label{inc-prev:eq:VsumRt}
\end{align}
by the properties of the binomial distribution (see \cref{E-distributions}).

The first moment follows.
\begin{align}
\E(P_t \mid \vec{Z})
  &= \E\left(\sum_{i=0}^{d_\text{max}-1} \left( Z_{t-i} - \sum_{j=0}^{i-1} R_{t-i,t-i+j} \right) \mid \vec{Z} \right) &\text{by \cref{inc-prev:eq:Pt-to-Rtt}}\\
  &= \sum_{i=0}^{d_\text{max}-1} \left( Z_{t-i} - \sum_{j=0}^{i-1} \E( R_{t-i,t-i+j} \mid \vec{Z}) \right) \\
  &= \sum_{i=0}^{d_\text{max}-1} \left( Z_{t-i} - Z_{t-i} F_D(i) \right) &\text{by \cref{inc-prev:eq:EsumRt}}\\
  &= \sum_{i=0}^{d_\text{max}-1} Z_{t-i} (1 - F_D(i)) \\
  &= \sum_{i=0}^{d_\text{max}-1} Z_{t-i} S(i+1) \label{inc-prev:eq:EPt}
\end{align}
where $S(i+1) = 1-F_D(i) = \prob(D \geq i+1)$ is the \emph{survival function} of $D$.

The second moment is simplified by the independence.
\begin{align}
\V(P_t \mid \vec{Z})
  &= \V\left(\sum_{i=1}^{d_\text{max}-1} P_{t-i,t} \mid \vec{Z} \right) &\text{by \cref{inc-prev:eq:Pt-to-Ptt}} \\
  &= \sum_{i=1}^{d_\text{max}-1} \V\left(P_{t-i,t} \mid \vec{Z} \right) &\text{by the conditionl independence} \\
  &= \sum_{i=1}^{d_\text{max}-1} \V\left(\sum_{j=0}^{i-1} R_{t-i,t-i+j} \mid \vec{Z} \right) &\text{by~\cref{inc-prev:eq:Ptt-to-R}}\\
  &= \sum_{i=1}^{d_\text{max}-1} Z_{t-i} F_D(i) (1 - F_D(i)) &\text{by~\cref{inc-prev:eq:VsumRt}} \\
  &\leq \sum_{i=1}^{d_\text{max}-1} Z_{t-i} (1 - F_D(i)) &\text{as $F_D \leq 1$}\\
  &= \E(P_t \mid \vec{Z}). \label{inc-prev:eq:boundVPt}
\end{align}

% The probability of being positive can be modelled in different ways depending on what assumptions are most reasonable.
% The most important assumption from a statistical perspective is the whether the probability of testing positive on a given day is independent conditional on the infection time.

% \todo[inline]{Is the discussion of this model just a distraction?}
% Assuming that the probability of testing positive depends only on the infection time implies no individual variation.
% This model is formalised as $Pr(Z_i(t) = 1 \mid B_i, Z_i(1), Z_i(2) \dots) = Pr(Z_i(t) \mid B_i) = p_{t-B_i}$, that is the probability of individual $i$ testing positive $t$ days after infection depends only on the infection start time and no other quantities.
% A slight relaxation of this model is introducing covariates, allowing some variation based on an individual's characteristics.

% The model with the most dependence between times, while remaining realistic, is a multi-state model.
% Therefore, if an individual has tested negative before the present time $t$ but after $B_i$ then they will test negative at time $t$.
% The complexity of the transitions between positive and negative can be very complex.
% I consider a slight extension to allow for the possibility of false negatives, meaning a negative test between $B_i$ and $E_i$, and that this probability may change.
% This leads to the following model.
% \begin{align}
%   \prob(Z_i(t) = 1 \mid B_i, \leq E_i) &= \begin{cases}
%     \psens(t - B_i) &B_i \leq t \leq E_i\\
%     0 &\text{otherwise}
%   \end{cases}\\
%   \prob(E_i = t \mid B_i) = f_i(B_i)
% \end{align}
% Conditional on both $B_i$ and $E_i$, then $Z_i(t)$ and $Z_i(t')$ are independent for $t \neq t'$.
% However, unlike the previous model, conditioning only on $B_i$ is not sufficient for $Z_i(t)$ and $Z_i(t')$ to be independent because each $Z_i(t)$ provides information on $E_i(t)$ (in particular, $Z_i(t) = 1$ implies $E_i > t$).
% For SARS-CoV-2, the latter model is more appropriate, although, as we shall see, the assumption that $\psens$ is independent of $t$ is violated.

The expectation in \cref{inc-prev:eq:EPt} is a discrete convolution equation.
The equation is of the same as the equation used for HIV/AIDS backcalculation, which itself is often found in a variety of applications~\autocite[and references therein]{brookmeyerBackcalculation}.
The system can be written in the form $\E(\vec{P} \mid \vec{Z}) = \matr{L} \vec{Z}$ where $\matr{L}$ is a lower-triangular matrix.
Numerical methods for solving systems of this form, that is solving for $\vec{Z}$ given $\vec{P}$, are well-established~\autocite[e.g.:][section 8.2]{highamAccuracy}.
However, these algorithms can be unstable, meaning that numerical errors are large.
This is a particular issue if $S(1)$ is small.
Since, by definition, $S(1)=1$ here, this is not an issue in this context.
%However, if instead of estimating the incidence of PCR positives, the aim was to estimate the incidence of infections, then $S(1)$ would be small and this would be an issue.
%While this quantity is more epidemiologically relevant, the time from infection to PCR positivity is short and, for now, I will neglect the difference.

The variance in \cref{inc-prev:eq:boundVPt} is upper bounded by \cref{inc-prev:eq:EPt}, a fact that will prove useful in \cref{inc-prev:sec:observation-process}.
\Cref{inc-prev:eq:EPt} shows that knowledge of the survival function, $S$, is crucial to the prevalence process.

\section{Observation process} \label{inc-prev:sec:observation-process}

% A \emph{prevalence survey} is a survey that tests a sample of the population for the presence of an infection.
% Series of prevalence surveys are the form of measuring prevalence that this thesis is concerned with.
% This thesis aims to infer the incidence rate from prevalence surveys.
% I use Bayesian inference, therefore the data contributes through the likelihood.
The previous sections have explained the processes leading to the population prevalence.
However, we do not directly observe the population prevalence but a sample of it.
The sampling is described mathematically by the observation process.
The survey design considered here reflects the CIS, although is applicable to many randomly sampled prevalence survey with samples for all $t$.

On day $t$ (for $t = 1, \dots, T$) a prevalence survey such as the CIS samples $n_t$ individuals at random from the population of interest and tests if they are prevalent.
Therefore, I model the number of positive tests, $X_t$, as $X_t \mid P_t, n_t, \Npop \dist \Bin(n_t, P_t/\Npop)$.
The conditioning on $n_t$ and $\Npop$ is implicit in what follows.
These quantities are assumed to be fixed by the study design and population choice, and independent of $P_t$ and $Z_t$.

The $X_t$s are conditionally independent, given the population prevalences at each time, $\vec{P} = (P_1, \dots, P_t)^T$. 
%For now, I assume that the sample is chosen uniformly at random from the population of interest.
%Assume that the surveys are independent of each other and $\vec{Z}$, conditional on the population prevalence at that time.
%The population prevalence is a latent quantity.
Prevalence surveys are used in this thesis to infer incidence.
I base this inference on the likelihood, $p(\vec{x} \mid \vec{Z})$.% = \int p(\vec{x} \mid \vec{P}) p(\vec{P} \mid \vec{Z}) d\vec{P} = \int \prod_t p(x_t \mid P_t) p(\vec{P} \mid \vec{Z}) d\vec{P}$, where $\vec{x} = (x_1, x_2, \dots)^T$.
Here $\vec{x} = (x_1, \dots, x_T)$ is the vector of observed number of positive tests in the prevalence survey on each day and $\vec{Z} = (Z_{-\dmax}, Z_{-\dmax+1} \dots, Z_T)$ is the vector of incidence on each day.
The incidence back to day $-\dmax$ will affect $P_1$ and therefore need to be included in $\vec{Z}$. 

This section argues that $p(\vec{x} \mid \vec{Z}) \approx \prob(\vec{X} = \vec{x} \mid \vec{P} = \E(\vec{P} \mid \vec{Z}))$.
As the $x_t$s are independent conditional on $\vec{P}$, it follows that $p(\vec{x} \mid \vec{Z}) \approx \prod_t \prob(X_t = x_t \mid P_t = \E(P_t \mid \vec{Z}))$.
This relationship will make inference for $\vec{Z}$ much simpler because the latent vector $\vec{P}$ and the dependence of the $x_t$s can be ignored.

The basis of the argument is that the first two moments of $\vec{x} \mid \vec{Z}$ and $\vec{X} \mid \vec{P} = \E(\vec{P} \mid \vec{Z})$ have negligible difference.
The first moment follows directly from the tower property of expectations.
The majority of the argument is to show that the variance-covariance matrix $\V(\vec{X} \mid \vec{Z}) \approx \V(\vec{X} \mid \vec{P} = \E(P \mid Z))$.
There are two steps to this argument.
I start with the diagonal elements, the variances.
I show that $\V(X_t \mid \vec{Z}) \approx \V(X_t \mid \vec{P} = \E(\vec{P} = \vec{Z}))$.
The conditional independence of the $x_t$s implies that the conditional covariances are 0.
Therefore, I show that the correlation between $X_t$ and $X_{t'}$ conditional only on $\vec{Z}$ is negligible.

Starting with the variances, by the law of total variance:
\begin{align}
  &\var\left( X_t \mid \vec{Z} \right) \\
    &= \var\left[\E\left( X_t \mid \vec{Z}, \vec{P} \right) \mid \vec{Z} \right] + \E\left[\var\left( X_t \mid \vec{Z}, \vec{P} \right) \mid \vec{Z} \right] \\
    &= \var\left( n_t \frac{P_t}{\Npop} \mid \vec{Z} \right) + \E\left( n_t \frac{P_t}{\Npop} \left(1 - \frac{P_t}{\Npop} \right) \mid \vec{Z} \right) \\
    &= \left( \frac{n_t}{\Npop} \right)^2 \var\left(P_t \mid \vec{Z} \right) + \frac{n_t}{\Npop} \E\left(P_t \mid \vec{Z} \right)  - \frac{n_t}{\Npop^2} \E\left(P_t^2 \mid \vec{Z} \right) \\
    &= \left( \frac{n_t}{\Npop} \right)^2 \var\left(P_t \mid \vec{Z} \right) + \frac{n_t}{\Npop} \E\left(P_t \mid \vec{Z} \right)  - \frac{n_t}{\Npop^2} \left(\E\left(P_t \mid \vec{Z} \right) ^ 2 + \V(P_t \mid \vec{X})\right) \\
    &= \frac{n_t(n_t - 1)}{\Npop^2} \var\left(P_t \mid \vec{Z} \right) + n_t \frac{\E\left(P_t \mid \vec{Z} \right)}{\Npop}\left(1 - \frac{\E\left(P_t \mid \vec{Z} \right)}{\Npop} \right) \\
    &= \frac{n_t(n_t - 1)}{\Npop^2} \var\left(P_t \mid \vec{Z} \right) + \V(X_t \mid P_t = \E(P_t \mid \vec{Z})).
\end{align}
The first term of this expression is negligible in the contexts I consider.
These contexts have $n_t << \Npop$.
$\Npop$ is the population of England (around 56 million) while $n_t$ is the daily sample size (10s of thousands).
Further $\V(P_t \mid \vec{Z}) \leq \E(P_t \mid \vec{Z})$ (\cref{inc-prev:eq:boundVPt}).
Therefore, $\var\left( X_t \mid \vec{Z} \right) \approx \V(X_t \mid P_t = \E(P_t \mid \vec{Z}))$.

A similar argument will show that the correlation between $x_t$ and $x_{t'}$ ($t \neq t'$) is negligible.
The basic structure of this argument is as the previous one.
The result follows because the observation noise dominates, and this is uncorrelated.
In the following, all are implicitly conditioned on $\vec{Z}$.
However, for clarity this is omitted from the notation.
\begin{align}
  &\lvert \cor(X_t, X_{t'}) \rvert \\
  &= \frac{\lvert\cov(X_t, X_{t'})\rvert}{\sqrt{\var(X_t) \var(X_{t'})}} &\text{by definition}\\
  &= \frac{\lvert \E(\cov(X_t, X_{t'} \mid \vec{P})) + \cov(\E(X_t \mid \vec{P}), \E(X_{t'} \mid \vec{P})) \rvert}{\sqrt{\var(X_t) \var(X_{t'})}} &\text{by law of total covariance} \\
  &= \frac{\lvert 0 + \cov(n_t P_t / \Npop, n_{t'} P_{t'} / \Npop) \rvert}{\sqrt{\var(X_t) \var(X_{t'})}} \\
  &= \frac{n_t n_{t'} \lvert \cov(P_t, P_{t'}) \rvert}{\Npop^2 \sqrt{\var(X_t) \var(X_{t'})}}  \\
  &<< \frac{n_t n_{t'} \lvert \cov(P_t, P_{t'}) \rvert}{\Npop^2 \sqrt{\var(P_t) \var(P_{t'})}} &\text{as $\V(X_t) >> \V(P_t)$} \\
  &= \frac{n_t n_{t'}}{\Npop^2} \lvert \cor(P_t, P_{t'}) \rvert \\
  &\leq \frac{n_t n_{t'}}{\Npop^2}.
\end{align}
$n_t n_{t'} / \Npop^2$ is very small, for the same reason that $n_t / \Npop$ is small.
The argument above shows that $\lvert \cor(X_t, X_{t'}) \rvert$ is much smaller than this.
Therefore, neglecting the correlation between $X_t$ and $X_{t'}$ is a good approximation.

This section showed that, when using a prevalence survey such as CIS, the likelihood $p(\vec{x} \mid \vec{Z})$ is very well approximated by $\prob(\vec{X} = \vec{x} \mid \vec{P} = \E(\vec{P} \mid \vec{Z}))$.


\section{Markov chain Monte Carlo} \label{inc-prev:sec:MCMC}

MCMC is a class of algorithms for sampling from a distribution where the normalising constant for the distribution is unknown.
I will use it to sample a set of parameters, say $\psi$ (which may be a scalar or vector), from a posterior distribution $p(\psi \mid y)$ where $y$ is the data.
MCMC algorithms produce a sample $\psi^1, \psi^2, \dots, \psi^M$ from $p(\psi \mid \vec{y})$ by constructing a Markov chain with $p(\psi \mid \vec{y})$ as its stationary distribution~\autocite[275]{gelmanBDA}.
It can be proved that as $M\to\infty$, the sample tends towards a sample from the stationary distribution, here the target posterior.
This section outlines some basic principles of MCMC and considerations when using it in practice.
Any reader interested in more technical details or proofs of the results here should consult the references within this section.

The Metropolis-Hastings algorithm~\autocite{hastingsMCMC} describes a general method for constructing such a Markov chain.
The algorithm requires a proposal distribution $q(\psi' \mid \psi)$, which proposes moving to a new state $\psi'$ from the current state $\psi$, and a calculation of $p(\psi \mid \vec{y})$ up to some constant, $\pi(\psi)$.
The proposal distribution is most commonly a Normal distribution centred on $\psi$, this is used in \cref{E-SEIR}.
The proposal is then either accepted or rejected based on the \emph{acceptance ratio}, which is computed using $\pi$.
The full algorithm is given in \cref{inc-prev:algorithm:MH}, with further details in available in a wide-range of sources~\autocites[e.g.][]{brooksMCMCNotes}[chapter 11]{gelmanBDA}.
\begin{algorithm}
 set $\psi^0$ to some initial value\;
 \For{$i = 1, \dots, M$}{
  sample a proposal $\psi' \sim q(\psi' \mid psi)$ \;
  calculate the acceptance probability $\alpha(\psi, \psi') = \min \left( 1, \frac{\pi(\psi') q(\psi \mid \psi')}{\pi(\psi) q(\psi' \mid \psi)} \right)$ \;
  with probability $\alpha(\psi, \psi')$ set $\psi^i$ to $\psi'$, otherwise set $\psi^i$ to $\psi^{i-1}$ \;
 }
 \caption{The Metropolis-Hastings algorithm.}
 \label{inc-prev:algorithm:MH}
\end{algorithm}

\todo[inline]{Talk about HMC and NUTS}

Two practical issues arise when applying MCMC: \emph{convergence} and \emph{stability}\todo{cite BUGS course}.

Convergence is how quickly the Markov chain approaches the stationary distribution.
Early draws from the Markov chain reflect the choice of the starting point, $\psi^0$, rather than the stationary distribution that the chain will eventually converge to~\autocite[282]{gelmanBDA}.
The simplest technique for assessing convergence is graphically, using traceplots~\autocite[81]{brooksMCMCNotes}.
Traceplots show the value of $\psi^i$ against $i$.
If the chain has converged, then the traceplot will show a random walk around the stationary distribution.
This can be improved by using multiple chains with different starting points~\autocite[283]{gelmanBDA}.
If both chains have converged, then they will be in the same location.

A more complex measure of convergence when running multiple MCMC chains is comparing the within-chain variance to the between-chains variance.
The principle here is that if all chains have converged then they are sampling from the same distribution.
This means that the within-chain and between-chain variances are the same~\autocite[82]{brooksMCMCNotes}.
In particular, I will make use of the improved Rhat statistic~\autocite{vehtariRhat}.
%The Rhat statistic is defined for a scalar parameter $\psi$.
%Let $\psi^{(ij)}$ denote the $i$th (of $M$) sample from the $j$th chain (of $N$).
%Let $\bar{\psi}^j$ denote the mean value of the samples of $\psi$ from the $j$th chain, $\bar{\psi}^j = \frac{1}{M} \sum_{i=1}^M \psi^{(ij)}$.
%Let $\bar{\psi}$ denote the mean value of the samples of $\psi$ from all chains, $\bar{\psi} = \frac{1}{N} \sum_{j=1}^N \bar{\psi}^j$.
%Then the between-chain variance is $B = \frac{M}{N-1} \sum_{j=1}^N (\bar{\psi}^j - \bar{\psi})^2$.
%The within-chain variance is $W = \frac{1}{N} \sum_{j=1}^N \frac{1}{M-1} \sum_{i=1}^M (\psi^{(ij)} - \bar{\psi}^j)^2$.
A Rhat of 1 means that the within-chain and between-chain variance are equal, informally that convergence is perfect, as far as can be measured by this technique.
Various rules-of-thumb exist for what should be considered an acceptable Rhat.
Historically, 1.1 has commonly been recommended, although more recent work suggests that 1.01 is more appropriate~\autocite{vehtariRhat}.

The stability of estimates considers the error arising because only a finite sample of the distribution of interest is available.
Furthermore, the sample will tend to be autocorrelated meaning that estimates using the example have higher variance than if it were an iid sample from the posterior~\autocite[286]{gelmanBDA}.
The \emph{effective sample size} corrects for the autocorrelation.
The effective sample size can be interpreted as the number of iid samples that would have the same variance as the MCMC sample~\autocite[286]{gelmanBDA}{vehtariRhat}.
Various techniques have been proposed for computing the effective sample size, I will follow the approach of \textcite{vehtariRhat}.

\section{Conclusion} \label{inc-prev:sec:conclusion}

Robust, unbiased prevalence estimates already exist (see\todo{ref prevalence estimates}), and incidence estimates are the objective of this thesis.
This chapter has shown that the key quantity relating these quantities is the distribution of the duration of PCR positivity.
However, no estimates of this quantity in the general population exist.
Therefore, I now turn to estimating the duration of PCR positivity.

\ifSubfilesClassLoaded{
  \listoftodos
}{}

\end{document}